\documentclass[12pt, letterpaper, onside]{article}
\usepackage[affil-it]{authblk} % author institution
\usepackage{amsmath} % math
\usepackage{amssymb} % math symbol
\usepackage[backend=biber]{biblatex}
\usepackage{graphicx}

\addbibresource{reference.bib}

\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]

\newcommand
{\point}
[1]
{\left(#1\right)}

\newcommand
{\twoFunc}
[1]
{#1\point{x, y}}

\newcommand
{\derv}
{\mathrm{d}}

\newcommand
{\fd} % [f]irst [d]erivative
[3]
{\frac{\derv#1}{\derv#2}\point{#3}}

\newcommand
{\sd} % [s]econd [d]erivative
[3]
{\frac{\derv^{2}#1}{\derv#2^{2}}\point{#3}}

\newcommand
{\fpd} % [f]irst [p]artial [d]erivative
[3]
{\frac{\partial{#1}}{\partial{#2}}\point{#3}}

\newcommand
{\spd} % [s]econd [p]artial [d]erivative
[4]
{\frac{\partial}{\partial{#3}} \left(\frac{\partial{#1}}{\partial{#2}}\right) \point{#4}}

\newcommand
{\appCon} % h and k approach condition
[2]
{\text{if } h \to 0^{#1} \text{, } k \to 0^{#2}}

\newcommand
{\counterFunc}
{
\begin{equation*}
    \twoFunc{f} = 
    \begin{cases}
        \twoFunc{\phi} & \text{if }x = 2y\text{,}\\
        \twoFunc{\psi} & \text{otherwise}\text{;}
    \end{cases}
\end{equation*}
where
\begin{align*}
    \twoFunc{\phi} &= x^{2} + y^{2} - 8x - 4y + 10\text{,}\\
    \twoFunc{\psi} &= x^{2} + y^{2} - 4x - 2y
\end{align*}
}

\newcommand
{\mat}
[1]
{\mathrm{#1}}

\newcommand
{\df} % [d]irectional [f]unction
[1]
{\xi_\vec{#1}}

\newcommand
{\R}
{\mathbb{R}}

\newcommand
{\itvec} % [i]nline 2d [vec]tor *inkscape*
[2]
{\left[\begin{array}{c}#1\\#2\end{array}\right]}

\newcommand
{\dirgraph} % [d]irection annotation for [graph] at (x, y) = (2, 1) *inkscape*
[2]
{\itvec{x}{y} = \itvec{2}{1} + k\itvec{#1}{#2}}

\renewcommand{\vec}{\mathbf}
\renewcommand{\Rn}{\mathbb{R}^n}

\title{\textbf{Ridge Problem in Stochastic Gradient Descent: An Analytical Solution}}
\author{Tran Phong Binh\thanks{Email: \texttt{phongbinh2511@gmail.com}}}
\affil{Department of Computer Science, National Tsing Hua University}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
% TODO
\end{abstract}

\section{Introduction}
The second partial derivative test is a renowed technique for determining local extremum points. This mathematical method had been well studied since the beginning era of calculus, and has been used extensively both in the academia and in real-world applications. Yet, upon studying the ridge problem of hill climbing\cite{ridge}, we stumbled on a thought that the same issue could present in the forementioned test under a stochastic space setup.

In this work, we prove that such problem exists, proposing an analytical solution, and check our approach on two multivariable functions. We hope our finding further pushes knowledge boundaries of stochastic gradient descent in specific, and optimization in general. With this, researchers and practitioners can now surpass false local extremum points to navigate down true local optimization objectives, consolidating the robustness of their theories and systems.

\section{Proof}
We prove that ridge problem presents in the second partial derivative test by analyzing the multivariable function
\counterFunc
at point $\point{2, 1}$.

\subsection{First Partial Derivatives}
We begin by examining the first derivatives of our function $f$ at point $\point{2, 1}$. Firstly, it is given that
\begin{equation*}
    \fpd{f}{x}{2, 1} = \lim_{h \to 0}{\frac{f\point{2 + h, 1} - f\point{2, 1}}{h}}\text{.}
\end{equation*}
Because the components on the numerator are
\begin{align*}
    f\point{2 + h, 1} & = \psi\point{2 + h, 1}\text{,}\\
        f\point{2, 1} & = \phi\point{2, 1}\\
                      & = -5\\
                      & = \psi\point{2, 1}\text{,}
\end{align*}
it holds that
\begin{align}
    \label{eq:proveSpd} % [prove] [s]econd [p]artial [d]erivatives
    \fpd{f}{x}{2, 1} & = \fpd{\psi}{x}{2, 1}\nonumber\\
                     & = 2x - 4\nonumber\\
                     & = 0\text{.}
\end{align}
Similarly,
\begin{align*}
    \fpd{f}{y}{2, 1} & = \fpd{\psi}{y}{2, 1}\\
                     & = 2y - 2\\
                     & = 0\text{.}
\end{align*}
Hence
\begin{equation}
    \label{zeroGradient}
    \nabla_{f}\point{2, 1} =
    \begin{bmatrix}
        0\\
        0
    \end{bmatrix}\text{.}
\end{equation}

\subsection{Second Partial Derivatives}
We then consider the second partial derivatives of our function at the same point.

\subsubsection{Second Partial Derivative With Respect To $x$}
We start with $f_{xx}\point{2, 1}$:
\begin{equation*}
    \spd{f}{x}{x}{2, 1} = \lim_{h \to 0}{\frac{\fpd{f}{x}{2 + h, 1} - \fpd{f}{x}{2, 1}}{h}}\text{.}
\end{equation*}
Assessing the first component of the numerator
\begin{equation}
    \fpd{f}{x}{2 + h, 1} = \lim_{k \to 0}{\frac{f\point{2 + h + k, 1} - f\point{2 + h, 1}}{k}}
\end{equation}
We observe that
\begin{align}
    f\point{2 + h + k, 1} & =
    \begin{cases}
        \psi\point{2 + h + k, 1} & \appCon{-}{-}\\
        \phi\point{2, 1} = -5 = \psi\point{2, 1} & \appCon{-}{+}\\
        \phi\point{2, 1} = -5 = \psi\point{2, 1} & \appCon{+}{-}\\
        \psi\point{2 + h + k, 1} & \appCon{+}{+}
    \end{cases}\\
    & = \psi\point{2 + h + k, 1}
\end{align}
and
\begin{equation}
    f\point{2 + h, 1} = \psi\point{2 + h, 1}
\end{equation}
Hence
\begin{equation}
    \label{eq:proveFxx}
    \fpd{f}{x}{2 + h, 1} = \fpd{\psi}{x}{2 + h, 1}
\end{equation}
By equations \ref{eq:proveSpd} and \ref{eq:proveFxx},
\begin{align}
    \spd{f}{x}{x}{2, 1} & = \spd{\psi}{x}{x}{2, 1}\\
                        & = 2
\end{align}

\subsubsection{Second Partial Derivative With Respect To $x$ Then $y$}
We continue by examining $f_{yx}\point{2, 1}$:
\begin{equation}
    \spd{f}{x}{y}{2, 1} = \lim_{h \to 0}{\frac{\fpd{f}{x}{2, 1 + h} - \fpd{f}{x}{2, 1}}{h}}
\end{equation}
Again, we assess the first component of the numerator
\begin{equation}
    \fpd{f}{x}{2, 1 + h} = \lim_{k \to 0}{\frac{f\point{2 + k, 1 + h} - f\point{2, 1 + h}}{k}}
\end{equation}
Similar to $f_{xx}\point{2, 1}$, we observe that
\begin{align}
    f\point{2 + k, 1 + h} & =
    \begin{cases}
        \psi\point{2 + k, 1 + h} & \appCon{-}{-}\\
        \psi\point{2 + k, 1 + h} & \appCon{-}{+}\\
        \psi\point{2 + k, 1 + h} & \appCon{+}{-}\\
        \psi\point{2 + k, 1 + h} & \appCon{+}{+}
    \end{cases}\\
    & = \psi\point{2 + k, 1 + h}
\end{align}
and
\begin{equation}
    f\point{2, 1 + h} = \psi\point{2, 1 + h}
\end{equation}
Hence
\begin{equation}
    \label{eq:proveFyx}
    \fpd{f}{x}{2, 1 + h} = \fpd{\psi}{x}{2, 1 + h}
\end{equation}
By equations \ref{eq:proveSpd} and \ref{eq:proveFyx},
\begin{align}
    \spd{f}{x}{y}{2, 1} & = \spd{\psi}{x}{y}{2, 1}\\
                        & = 0
\end{align}

\subsubsection{Hessian Determinant}
Performing similar deductions for $f_{xy}\point{2, 1}$ and $f_{yy}\point{2, 1}$, we have
\begin{align}
    f_{xx}\point{2, 1} & = 2\\
    f_{yx}\point{2, 1} & = 0\\
    f_{xy}\point{2, 1} & = 0\\
    f_{yy}\point{2, 1} & = 2
\end{align}
Hence the Hessian determinant
\begin{align}
    H_{f} & = f_{xx}\point{2, 1}f_{yy}\point{2, 1} - f_{yx}\point{2, 1}f_{xy}\point{2, 1}\\
    \label{positiveDetH}
      & = 4 > 0
\end{align}

\subsection{Second Partial Derivative Test Contradiction}
According to \cite{secondDerivativeTest}, by equations \ref{zeroGradient}, \ref{positiveDetH}, and the fact that $f_{xx}\point{2, 1} = 2 > 0$, we shall conclude $(2, 1)$ is a local minimum point. However, this is incorrect, as for $h \in \R$, $h \to 0^{+}$:
\begin{align}
    f\point{2 + 2h, 1 + h} & = \phi\point{2 + 2h, 1 + h}\\
                           & < \phi\point{2, 1} = f\point{2, 1}
\end{align}
That is, $\point{2, 1}$ is not a local minimum point (proof in appendix). % TODO appendix

\section{Intuition}
\begin{figure}[h]
    \centering
    \def\svgwidth{0.6\textwidth}
    \input{f_graph.pdf_tex}
    \caption{Partial graph of the function $\twoFunc{f}$}
    \label{fig:f}
\end{figure}

Considering our function's graph depicted in Figure \ref{fig:f} at point $\point{2, 1}$, we observe that the first derivatives in the $x$ and $y$ directions are zeroes, making the zero gradient. Furthermore, the curvature of the function is up in all directions at the point of interest, indicating the positive definite Hessian. With this premise, the second partial derivative test concludes that $\point{2, 1}$ is a local minimum point.

However, this is analytically and visually false. Even though the curvature along the $\vec{v} = \point{4, 2}$ direction is up, the corresponding first derivative differs from zero, invalidating the second partial derivative test.

\section{Solution}
For the tangent hyperplane of a multivariable function at a point to be flat, it is not enough for the gradient of the function at that point to be the zero vector, but the first derivatives of the function at the point of interest must equal to zero in all directions.

In this work, we provide an analytical solution to this issue by redefining what it takes for the tangent hyperplane of a function at a point to be flat. Firstly, we define the concept of directional function:
\begin{definition}
    Given a multivariable function
    \begin{align*}
        f \colon \Rn & \longrightarrow \R\\
                        \vec{x} & \longmapsto f\point{\vec{x}}
    \end{align*}
    where $\vec{x} = \point{x_{1}, x_{2}, \dots, x_{n}}$. For a direction\footnote{We exclude the vector $\vec{0}$ from all of our discussions on directional function.} $\vec{v} \in \Rn$, the directional function of $f$ at point $\vec{x_{0}}$ is the single variable function
    \begin{align*}
        \df{v} \colon \R & \longrightarrow \R\\
                                    k & \longmapsto f\point{\vec{x_{0}} + k\vec{v}}
    \end{align*}
\end{definition}
We are now ready to redefine flat tangent hyperplane:
\begin{definition}
    Given a multivariable function $f$, its tangent hyperplane is flat at point $\vec{x_{0}}$ iff for all directions $\vec{v} \in \Rn$, the directional function $\df{v}$ at that point has zero first derivative at $k = 0$ i.e.\ iff $\forall \vec{v} \in \Rn \colon \fd{\df{v}}{k}{0} = 0$.
\end{definition}
From this definition, we can derive our new local mininimum test:
\begin{theorem}
    \label{th:localMin}
    Given a multivariable function $f$, if its tangent hyperplane is flat at point $\vec{x_{0}}$, and for all directions $\vec{v} \in \Rn$, the directional function $\df{v}$ at that point has positive second derivative at $k = 0$ i.e.\ if $\forall \vec{v} \in \Rn \colon \fd{\df{v}}{k}{0} = 0 \land \sd{\df{v}}{k}{0} > 0$, then $\vec{x_{0}}$ is a local minimum point.
\end{theorem}

\subsection{False Local Minimum}
We revisit our first example to see if the test works:
\counterFunc
For the direction $\vec{v} = \point{4, 2}$, our directional function at $\vec{x_{0}} = \point{2, 1}$ is
\begin{align}
    \df{v}\point{k} & = f\point{\vec{x_{0}} + k\vec{v}}\\
    & = f\point{
    \begin{bmatrix}
        2\\
        1
    \end{bmatrix}
    + k
    \begin{bmatrix}
        4\\
        2
    \end{bmatrix}
    }\\
    & = f\point{
    \begin{bmatrix}
        2 + 4k\\
        1 + 2k
    \end{bmatrix}
    } \text{ or } f\point{2 + 4k, 1 + 2k}
\end{align}
Since $x = 2y$,
\begin{align}
    \df{v}\point{k} & = \phi\point{2 + 4k, 1 + 2k}\\
    & = \left(2 + 4k\right)^{2} + \left(1 + 2k\right)^{2} - 8\left(2 + 4k\right) - 4\left(1 + 2k\right) + 10\\
    & = 4 + 16k + 16k^{2} + 1 + 4k + 4k^{2} - 16 - 32k - 4 - 8k + 10\\
    & = 20k^{2} - 20k - 5
\end{align}
We examine the directional function's first derivative at $k = 0$:
\begin{align}
    \fd{\df{v}}{k}{0} & = 40k - 20\\
    & = -20 \neq 0
\end{align}
Hence $\vec{x_{0}} = (2, 1)$ is not a local minimum point!

\subsection{True Local Minimum}
We consolidate our test with the well-known function
\begin{equation}
    f\point{\vec{x}} = \vec{x}^{\intercal}\mat{A}\vec{x}
\end{equation}
where $\underset{n \times n}{\mat{A}} \succ 0$. For any directions $\vec{v} \in \Rn$, our directional function at $\vec{x_{0}} = \vec{0}$ is
\begin{align}
    \df{v}\point{k} & = f\point{\vec{x_{0}} + k\vec{v}}\\
    & = \left(k\vec{v}\right)^{\intercal}\mat{A}\left(k\vec{v}\right)\\
    & = k^{2}\vec{v}^{\intercal}\mat{A}\vec{v}
\end{align}
We examine the directional function's first derivative at $k = 0$:
\begin{align}
    \fd{\df{v}}{k}{0} & = 2k\vec{v}^{\intercal}\mat{A}\vec{v}\\
    & = 0
\end{align}
Furthermore,
\begin{equation}
    \sd{\df{v}}{k}{0} = 2\vec{v}^{\intercal}\mat{A}\vec{v}
\end{equation}
Since $\mat{A} \succ 0$, $\forall \vec{v} \neq \vec{0} \colon \vec{v}^{\intercal}\mat{A}\vec{v} > 0$. Thus, our directional function's second derivative is positive. According to Theorem \ref{th:localMin}, $\vec{x_{0}} = \vec{0}$ is a local minimum point!

\section{Conclusion}

\section{Future Work}

\printbibliography

\end{document}
